# -*- coding: utf-8 -*-
"""food_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1geWXP6izwD1NddrJ8VE8JYVjLxYvit
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
# %matplotlib inline
from pandas import read_excel
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from scipy import stats
from sklearn.datasets import load_boston
import seaborn as scs

df = pd.read_excel("/content/dataset.xlsx")
df.head()

df.dropna()
df.rename(columns = {'Gas (ppm)':'gasdata'}, inplace = True)
df['id'] = range(1, len(df) + 1)
del df["Time"]
del df["Date"]
df.describe()
df.head()

plt.scatter(df.id,df['gasdata'])
plt.xlabel('id')
plt.ylabel('gasdata')



z_scores = stats.zscore(df)
# calculate z-scores of `df`

# abs_z_scores = np.abs(z_scores)
# filtered_entries = (abs_z_scores < 3).all(axis=1)
# new_df = df[filtered_entries]
# df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]
# df
# df[np.abs(df.gasdata-df.gasdata.mean()) <= (3*df.gasdata.std())]
# df[~(np.abs(df.gasdata-df.gasdata.mean()) > (3*df.gasdata.std()))]
# df

dx=df[df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]]
df

plt.scatter(df.id,df['gasdata'])
plt.xlabel('id')
plt.ylabel('gasdata')
zscore_df = pd.DataFrame(stats.zscore(df.fillna(df.mean())), index=df.index, columns=df.columns).where(df.notna())
zscore_df
df3 = zscore_df[~(zscore_df['gasdata'] >= 2)]  
df3

scs.boxplot(df3['gasdata'])

# z = (stats.zscore(df.gasdata))
# z
df['zscore'] = (df.gasdata - df.gasdata.mean())/df.gasdata.std(ddof=0)
df
df.zscore.describe()

zscore_df = pd.DataFrame(stats.zscore(df.fillna(df.mean())), index=df.index, columns=df.columns).where(df.notna())

zscore_df

# del df["id"]
# df
# del zscore_df['id']
zscore_df
zscore_df['id'] = range(1, len(df) + 1)
zscore_df

plt.scatter(zscore_df.id,zscore_df['gasdata'])
plt.xlabel('id')
plt.ylabel('gasdata')
zscore_df
df3

# zscore_df[(np.abs(stats.zscore(zscore_df)) < 3).all(axis=1)]
# zscore_df
# fe= (zscore_df.gasdata< 3)
# fe
# zscore_df.drop(zscore_df[zscore_df['gasdata'] <3].index, inplace = True)
# zscore_df
df3['ids'] = range(1, len(df3) + 1)
plt.scatter(df3.id,df3['gasdata'])
plt.xlabel('id')
plt.ylabel('gasdata')
df3.describe()

# Commented out IPython magic to ensure Python compatibility.
dfx = df3[:100]
dfy = df3[220:440]
dfz = df3[440:]
dfw=df3[100:]
import matplotlib.pyplot as plt
# %matplotlib inline
plt.xlabel('id')
plt.ylabel('gasdata')
# plt.scatter(dfx['id'], dfx['gasdata'],color="green",marker='+')
# plt.scatter(dfy['id'], dfy['gasdata'],color="blue",marker='.')
# plt.scatter(dfz['id'], dfz['gasdata'],color="gray",marker='.')
plt.scatter(dfw['id'], dfw['gasdata'],color="gray",marker='.')

# print(df3.to_string())
# dfw.dropna(how='all', inplace=True)




# dfy.drop([225,226], axis=0, inplace=True)
# print(dfy.to_string())
df3.drop([225,226], axis=0, inplace=True)
km = KMeans(n_clusters=3)
y_predicted = km.fit_predict(df3[['id','gasdata']])

df3['y_predicted']=y_predicted
df3

df4= df3[df3['y_predicted']==0]
df5= df3[df3['y_predicted']==1]
df6= df3[df3['y_predicted']==2]
plt.xlabel('id')
plt.ylabel('gasdata')
plt.scatter(df4['id'], df4['gasdata'],color="yellow",marker='+')
plt.scatter(df5['id'], df5['gasdata'],color="green",marker='.')
plt.scatter(df6['id'], df6['gasdata'],color="red",marker='*')

from sklearn.model_selection import train_test_split
# X=df3.y_predicted
# Y=df3.id

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=4)
# knn.fit(X_train, y_train)
Y=pd.DataFrame(df3.y_predicted)
# Y=np.array([df3.y_predicted])
# Y=pd.DataFrame(df3['ids'])
X=pd.DataFrame(df3.gasdata)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33,random_state=42)

# X_train
# y_train
# y_test
#knn.score(X_test, y_test)
model = neighbors.KNeighborsRegressor(n_neighbors = 10)
model.fit(X_train, y_train) 
l=np.array(-0.411123)
pred=model.predict(l.reshape(-1,1))
print(pred)

# knn.fit(X_train, y_train)



#import required packages

# rmse_val = [] #to store rmse values for different k
# for K in range(20):
#     K = K+1
#     model = neighbors.KNeighborsRegressor(n_neighbors = K)

#     model.fit(X_train, y_train)  #fit the model
#     pred=model.predict(X_test) #make prediction on test set
#     error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse
#     rmse_val.append(error) #store rmse values
#     print('RMSE value for k= ' , K , 'is:', error)

#plotting the rmse values against k values
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

# prediction=model.predict()



from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn import ensemble

# gds=1123
# x=np.array(gds)
# z_test=x.reshape((-1,1))
# model.predict(z_test)
# df3

# from sklearn.linear_model import  MultiClassLogisticRegression
# models = MultiClassLogisticRegression()
class MultiClassLogisticRegression:
        
    def fit(self, X, y, epochs=10):
        self.classes = np.unique(y)
        self.class_labels = {c:i for i,c in enumerate(self.classes)}
        y=self.one_hot(y)
        self.loss = []
        X = np.insert(X, 0, 1, axis=1)
        self.weights = np.zeros(shape=(len(self.classes), X.shape[1]))
        Xs = X.T.dot(X)
        for i in range(epochs):
            self.loss.append(self.cross_entropy(y, self.predict_(X)))
            h = self.predict_(X)
            invH =  np.linalg.pinv(Xs * np.sum(h.T.dot(1-h)))
            gradient = (h - y).T.dot(X)
            self.weights -= gradient.dot(invH)
            print(' Training Accuray at {} iterations is {}'.format(i+1, self.score_(X, y)))
        return self
    
    def predict(self, X):
        return self.predict_(np.insert(X, 0, 1, axis=1))
    
    def predict_(self, X):
        pre_vals = np.dot(X, self.weights.T).reshape(-1,len(self.classes))
        return self.softmax(pre_vals)
    
    def softmax(self, z):
        return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1,1)

    def predict_classes(self, X):
        return np.argmax(self.predict(X), axis=1)

    def one_hot(self, y):
        return np.eye(len(self.classes))[y.reshape(-1)]
    
    def one_hot(self, y):
        return np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]
    
    def score(self, X, y):
        return np.mean(self.predict_classes(X) == y)
    
    def score_(self, X, y):
        probs = self.predict_(X)
        return np.mean(np.argmax(probs, axis=1) == np.argmax(y, axis=1))
    
    def cross_entropy(self, y, probs):
        return (-1 / len(probs)) * (y * np.log(probs)).sum()

# models.fit(X_train, y_train)
# print(X_train.to_markdown())

lr = MultiClassLogisticRegression()

# lr.fit(X_train,y_train)
# X_train
# y_train



lr.fit(np.argmax(X_train.gasdata, axis=0),np.argmax(y_train.y_predicted, axis=0))





# models.score(X_test, y_test)

# models.predict(df3.gasdata[0:5])
# df3.gasdata[0:5]
dgs=pd.DataFrame([df3.gasdata[0:1]])
dgs
models.predict(dgs)

# Commented out IPython magic to ensure Python compatibility.
from sklearn import neighbors
from sklearn.metrics import mean_squared_error 
from math import sqrt
import matplotlib.pyplot as plt
# %matplotlib inline

